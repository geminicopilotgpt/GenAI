{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOrCsr+rsIdCXNQZOKYJi41",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geminicopilotgpt/GenAI/blob/main/Gpt2Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1yHzWJxYdyx",
        "outputId": "5502a3f6-744c-4134-cf71-4ed4f415178b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a short story about a life of 23 year old guy: http://nypost.com/2012/10/08/young-yale-writers-to-have-kids/\n",
            "\n",
            "We talk about it all.\n",
            "\n",
            "Some\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "prompt = \"Write a short story about a life of 23 year old guy:\"\n",
        "\n",
        "output = generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "prompt = \"Write a short story about the life of a 23-year-old guy.\"\n",
        "\n",
        "output = generator(prompt,\n",
        "                   max_length=200, # Increased max_length\n",
        "                   num_return_sequences=1,\n",
        "                   truncation=True) # Added explicit truncation\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3VEzwrcZ_4J",
        "outputId": "ab21fbfa-005c-4058-ae58-a46ca7a18ad4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a short story about the life of a 23-year-old guy.\n",
            "\n",
            "\n",
            "I've given my personal opinion on this and we should all use our voices now when we speak up about the real problems here; it's an important piece of work in which we are doing better rather than worse. We need to make the world better for people who know how much they care about this issue and who care about their kids because it is something that affects all lives, and what they care about means nothing.\n",
            "\n",
            "\n",
            "Now there are hundreds and thousands of kids in this country who are going through mental illness who aren't even aware that mental illness is a psychiatric condition but are just doing whatever it is they want to do. Let them know you care and they can move on.\n",
            "\n",
            "\n",
            "Thank you for sharing your experiences about how to start a conversation to help. There are many things to add for people who already care. And you won't be as hard on those who don't.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "prompt = \"Write a short story about a 23-year-old guy named Alex who is trying to start his own business in a big city.\"\n",
        "\n",
        "output = generator(prompt,\n",
        "                   max_length=200,\n",
        "                   num_return_sequences=1,\n",
        "                   truncation=True,\n",
        "                   temperature=0.7) # Lower temperature\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Av5FI9iaMYt",
        "outputId": "f9b03673-2df6-499a-be95-e6aec4ee1f5d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a short story about a 23-year-old guy named Alex who is trying to start his own business in a big city. A little something about that is kind of a cool touch.\n",
            "\n",
            "I've never had a story like that before. I think I was just trying to come up with the right character for the story. I don't know how they came up with Alex, but they did. I think they came up with the character for him because they wanted him to be a character who is part of the larger community.\n",
            "\n",
            "So I think that's the perfect fit for me. I love this character. I love working with that character. I love working with this character. I love working with that character.\n",
            "\n",
            "[Laughs.]\n",
            "\n",
            "It's funny because you've said some interesting things about the other two shows, The Walking Dead and Game of Thrones, but you're not really a guy.\n",
            "\n",
            "I don't really. I've never said that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "prompt = \"Write a short story about Alex, a 23-year-old, who moves to New York City to open a coffee shop. Describe his first day, the challenges he faces, and a small victory he achieves.\"\n",
        "\n",
        "output = generator(prompt,\n",
        "                   max_length=300,\n",
        "                   num_return_sequences=1,\n",
        "                   truncation=True,\n",
        "                   temperature=0.7)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQkPJv8iapOF",
        "outputId": "f9e95c63-bba4-4c07-8a24-7844becb1318"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a short story about Alex, a 23-year-old, who moves to New York City to open a coffee shop. Describe his first day, the challenges he faces, and a small victory he achieves. Write about his trip to the United States to visit his father, who died of brain cancer, and his struggle with depression, all while wearing a t-shirt with the message \"Keep a smile on your face, kids! I've got something to tell you.\"\n",
            "\n",
            "How do you feel about the whole process of getting started in comics?\n",
            "\n",
            "It was incredible. I had such a great time. I was completely overwhelmed. It was so overwhelming. I was so overwhelmed and I hadn't thought about a problem for a couple of months. I had to do it. I had to figure out how to do it.\n",
            "\n",
            "What was your first day like?\n",
            "\n",
            "I was so excited, it was so great to be part of something that I loved so much. It was amazing. I was so surprised by the amount of people that came. It was so much fun and really fun.\n",
            "\n",
            "What do you get out of the experience?\n",
            "\n",
            "I get so much. I get to see the world through an eyesore. I get to see what it is to be a human being. I get to hear the stories behind the characters. It was really crazy because of the amazing people that came to work with me. It's a really special environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import io\n",
        "\n",
        "def get_google_search_text(query, num_results=5):\n",
        "    \"\"\"Performs a Google search and returns the text content from the top results.\"\"\"\n",
        "    search_results = search(query, num_results=num_results)\n",
        "    all_text = \"\"\n",
        "    for url in search_results:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True) #Gets all text from the webpage.\n",
        "            all_text += text + \" \"\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {url}: {e}\")\n",
        "    return all_text\n",
        "\n",
        "def finetune_and_generate(search_query, prompt):\n",
        "    \"\"\"Performs a Google search, finetunes GPT-2, and generates text.\"\"\"\n",
        "\n",
        "    text_data = get_google_search_text(search_query)\n",
        "\n",
        "    if not text_data:\n",
        "        print(\"No search results or text found.\")\n",
        "        return\n",
        "\n",
        "    train_file_obj = io.StringIO(text_data)\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=\"in_memory\",\n",
        "        block_size=128,\n",
        "        file_obj=train_file_obj\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gpt2-finetuned\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "        prediction_loss_only=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(\"./gpt2-finetuned\")\n",
        "    tokenizer.save_pretrained(\"./gpt2-finetuned\")\n",
        "\n",
        "    finetuned_generator = pipeline('text-generation', model='./gpt2-finetuned')\n",
        "\n",
        "    output = finetuned_generator(prompt,\n",
        "                       max_length=300,\n",
        "                       num_return_sequences=1,\n",
        "                       truncation=True,\n",
        "                       temperature=0.7)\n",
        "\n",
        "    print(output[0]['generated_text'])\n",
        "\n",
        "# Example Usage:\n",
        "search_query = \"New York City coffee shop trends\"\n",
        "prompt = \"Write a short story about a coffee shop in New York City.\"\n",
        "finetune_and_generate(search_query, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "YLTkbAuRbBxZ",
        "outputId": "c1e01327-3343-4505-ee70-deee3b28bb6b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "search() got an unexpected keyword argument 'num_results'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6fa3be5e38a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0msearch_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"New York City coffee shop trends\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Write a short story about a coffee shop in New York City.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mfinetune_and_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-6fa3be5e38a1>\u001b[0m in \u001b[0;36mfinetune_and_generate\u001b[0;34m(search_query, prompt)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m\"\"\"Performs a Google search, finetunes GPT-2, and generates text.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_google_search_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtext_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-6fa3be5e38a1>\u001b[0m in \u001b[0;36mget_google_search_text\u001b[0;34m(query, num_results)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_google_search_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"Performs a Google search and returns the text content from the top results.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msearch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mall_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: search() got an unexpected keyword argument 'num_results'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade google-search-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rxXKuNPb7cJ",
        "outputId": "7269999b-6afe-4057-8599-050c58d51b64"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement google-search-python (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for google-search-python\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_google_search_text(query, num_results=5):\n",
        "    \"\"\"Performs a Google search and returns the text content from the top results.\"\"\"\n",
        "    search_results = search(query) #Removed num_results here.\n",
        "    all_text = \"\"\n",
        "    count = 0 #Added counter\n",
        "    for url in search_results:\n",
        "        if count >= num_results: #Added check\n",
        "            break\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            all_text += text + \" \"\n",
        "            count += 1 #Increment counter.\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {url}: {e}\")\n",
        "    return all_text"
      ],
      "metadata": {
        "id": "-MqsHdBBcSAR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import io\n",
        "\n",
        "def get_google_search_text(query, num_results=5):\n",
        "    \"\"\"Performs a Google search and returns the text content from the top results.\"\"\"\n",
        "    search_results = search(query)\n",
        "    all_text = \"\"\n",
        "    count = 0\n",
        "    for url in search_results:\n",
        "        if count >= num_results:\n",
        "            break\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            all_text += text + \" \"\n",
        "            count += 1\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {url}: {e}\")\n",
        "    return all_text\n",
        "\n",
        "def finetune_and_generate(search_query, prompt):\n",
        "    \"\"\"Performs a Google search, finetunes GPT-2, and generates text.\"\"\"\n",
        "\n",
        "    text_data = get_google_search_text(search_query)\n",
        "\n",
        "    if not text_data:\n",
        "        print(\"No search results or text found.\")\n",
        "        return\n",
        "\n",
        "    train_file_obj = io.StringIO(text_data)\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=\"in_memory\",\n",
        "        block_size=128,\n",
        "        file_obj=train_file_obj\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gpt2-finetuned\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "        prediction_loss_only=True,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(\"./gpt2-finetuned\")\n",
        "    tokenizer.save_pretrained(\"./gpt2-finetuned\")\n",
        "\n",
        "    finetuned_generator = pipeline('text-generation', model='./gpt2-finetuned')\n",
        "\n",
        "    output = finetuned_generator(prompt,\n",
        "                       max_length=300,\n",
        "                       num_return_sequences=1,\n",
        "                       truncation=True,\n",
        "                       temperature=0.7)\n",
        "\n",
        "    print(output[0]['generated_text'])\n",
        "\n",
        "# Example Usage:\n",
        "search_query = \"New York City coffee shop trends\"\n",
        "prompt = \"Write a short story about a coffee shop in New York City.\"\n",
        "finetune_and_generate(search_query, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "25Qp1kmFcV2n",
        "outputId": "476a05af-a3c6-43c5-c588-bc9139552876"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "TextDataset.__init__() got an unexpected keyword argument 'file_obj'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-87515498204a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0msearch_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"New York City coffee shop trends\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Write a short story about a coffee shop in New York City.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mfinetune_and_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-87515498204a>\u001b[0m in \u001b[0;36mfinetune_and_generate\u001b[0;34m(search_query, prompt)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     train_dataset = TextDataset(\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"in_memory\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TextDataset.__init__() got an unexpected keyword argument 'file_obj'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "7cEyAL6TcmMU",
        "outputId": "ee724112-e2c2-4f8f-dd75-b2be53882e23"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "Successfully installed transformers-4.49.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "4369b4d518c74d089530d1fab4e45f0e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tempfile #added import\n",
        "import os #added import\n",
        "\n",
        "def get_google_search_text(query, num_results=5):\n",
        "    # ... (your get_google_search_text function remains the same)\n",
        "\n",
        "def finetune_and_generate(search_query, prompt):\n",
        "    \"\"\"Performs a Google search, finetunes GPT-2, and generates text.\"\"\"\n",
        "\n",
        "    text_data = get_google_search_text(search_query)\n",
        "\n",
        "    if not text_data:\n",
        "        print(\"No search results or text found.\")\n",
        "        return\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    # Save to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp_file:\n",
        "        temp_file.write(text_data)\n",
        "        temp_file_path = temp_file.name\n",
        "\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=temp_file_path,\n",
        "        block_size=128\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gpt2-finetuned\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "        prediction_loss_only=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(\"./gpt2-finetuned\")\n",
        "    tokenizer.save_pretrained(\"./gpt2-finetuned\")\n",
        "\n",
        "    finetuned_generator = pipeline('text-generation', model='./gpt2-finetuned')\n",
        "\n",
        "    output = finetuned_generator(prompt,\n",
        "                       max_length=300,\n",
        "                       num_return_sequences=1,\n",
        "                       truncation=True,\n",
        "                       temperature=0.7)\n",
        "\n",
        "    print(output[0]['generated_text'])\n",
        "\n",
        "    # Clean up the temporary file\n",
        "    os.remove(temp_file_path)\n",
        "\n",
        "# Example Usage:\n",
        "search_query = \"New York City coffee shop trends\"\n",
        "prompt = \"Write a short story about a coffee shop in New York City.\"\n",
        "finetune_and_generate(search_query, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "4dx5SRyZcu3G",
        "outputId": "2f261f77-1471-47a6-a211-dcc73f9f1bf1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 8 (<ipython-input-1-3bf2a16fd7b7>, line 11)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3bf2a16fd7b7>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    def finetune_and_generate(search_query, prompt):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def get_google_search_text(query, num_results=5):\n",
        "    \"\"\"Performs a Google search and returns the text content from the top results.\"\"\"\n",
        "    search_results = search(query)\n",
        "    all_text = \"\"\n",
        "    count = 0\n",
        "    for url in search_results:\n",
        "        if count >= num_results:\n",
        "            break\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            all_text += text + \" \"\n",
        "            count += 1\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {url}: {e}\")\n",
        "    return all_text\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def get_google_search_text(query, num_results=5):\n",
        "    \"\"\"Performs a Google search and returns the text content from the top results.\"\"\"\n",
        "    search_results = search(query)\n",
        "    all_text = \"\"\n",
        "    count = 0\n",
        "    for url in search_results:\n",
        "        if count >= num_results:\n",
        "            break\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            all_text += text + \" \"\n",
        "            count += 1\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {url}: {e}\")\n",
        "    return all_text\n",
        "\n",
        "def finetune_and_generate(search_query, prompt):\n",
        "    \"\"\"Performs a Google search,\n",
        "    \"\"\"\n",
        "    # ... (rest of your function code)"
      ],
      "metadata": {
        "id": "JcphfC5LdF-G"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def get_google_search_text(query, num_results=5):\n",
        "    \"\"\"Performs a Google search and returns the text content from the top results.\"\"\"\n",
        "    search_results = search(query)\n",
        "    all_text = \"\"\n",
        "    count = 0\n",
        "    for url in search_results:\n",
        "        if count >= num_results:\n",
        "            break\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            all_text += text + \" \"\n",
        "            count += 1\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {url}: {e}\")\n",
        "    return all_text\n",
        "\n",
        "def finetune_and_generate(search_query, prompt):\n",
        "    \"\"\"Performs a Google search, finetunes GPT-2, and generates text.\"\"\"\n",
        "\n",
        "    text_data = get_google_search_text(search_query)\n",
        "\n",
        "    if not text_data:\n",
        "        print(\"No search results or text found.\")\n",
        "        return\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp_file:\n",
        "        temp_file.write(text_data)\n",
        "        temp_file_path = temp_file.name\n",
        "\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=temp_file_path,\n",
        "        block_size=128\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gpt2-finetuned\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "        prediction_loss_only=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(\"./gpt2-finetuned\")\n",
        "    tokenizer.save_pretrained(\"./gpt2-finetuned\")\n",
        "\n",
        "    finetuned_generator = pipeline('text-generation', model='./gpt2-finetuned')\n",
        "\n",
        "    output = finetuned_generator(prompt,\n",
        "                       max_length=300,\n",
        "                       num_return_sequences=1,\n",
        "                       truncation=True,\n",
        "                       temperature=0.7)\n",
        "\n",
        "    print(output[0]['generated_text'])\n",
        "\n",
        "    os.remove(temp_file_path)\n",
        "\n",
        "# Add this line to call the function and start the process\n",
        "search_query = \"New York City coffee shop trends\"\n",
        "prompt = \"Write a short story about a coffee shop in New York City.\"\n",
        "finetune_and_generate(search_query, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "X0IIN9ghdaCH",
        "outputId": "96439d8a-7da3-419a-d955-ae52261e3c65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 07:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-cca65007886b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0msearch_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"New York City coffee shop trends\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Write a short story about a coffee shop in New York City.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mfinetune_and_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-cca65007886b>\u001b[0m in \u001b[0;36mfinetune_and_generate\u001b[0;34m(search_query, prompt)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./gpt2-finetuned\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mfinetuned_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text-generation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./gpt2-finetuned'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     output = finetuned_generator(prompt,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments, pipeline #Added import of pipeline\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def get_google_search_text(query, num_results=5):\n",
        "    \"\"\"Performs a Google search and returns the text content from the top results.\"\"\"\n",
        "    search_results = search(query)\n",
        "    all_text = \"\"\n",
        "    count = 0\n",
        "    for url in search_results:\n",
        "        if count >= num_results:\n",
        "            break\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            all_text += text + \" \"\n",
        "            count += 1\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {url}: {e}\")\n",
        "    return all_text\n",
        "\n",
        "def finetune_and_generate(search_query, prompt):\n",
        "    \"\"\"Performs a Google search, finetunes GPT-2, and generates text.\"\"\"\n",
        "\n",
        "    text_data = get_google_search_text(search_query)\n",
        "\n",
        "    if not text_data:\n",
        "        print(\"No search results or text found.\")\n",
        "        return\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp_file:\n",
        "        temp_file.write(text_data)\n",
        "        temp_file_path = temp_file.name\n",
        "\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=temp_file_path,\n",
        "        block_size=128\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gpt2-finetuned\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "        prediction_loss_only=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(\"./gpt2-finetuned\")\n",
        "    tokenizer.save_pretrained(\"./gpt2-finetuned\")\n",
        "\n",
        "    finetuned_generator = pipeline('text-generation', model='./gpt2-finetuned')\n",
        "\n",
        "    output = finetuned_generator(prompt,\n",
        "                       max_length=300,\n",
        "                       num_return_sequences=1,\n",
        "                       truncation=True,\n",
        "                       temperature=0.7)\n",
        "\n",
        "    print(output[0]['generated_text'])\n",
        "\n",
        "    os.remove(temp_file_path)\n",
        "\n",
        "# Add this line to call the function and start the process\n",
        "search_query = \"New York City coffee shop trends\"\n",
        "prompt = \"Write a short story about a coffee shop in New York City.\"\n",
        "finetune_and_generate(search_query, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "hDKxjR-idiEa",
        "outputId": "7cadbb49-2428-4a96-b903-47f03d75d0cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 07:04, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a short story about a coffee shop in New York City.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "finetuned_generator = pipeline('text-generation', model='./gpt2-finetuned')\n",
        "\n",
        "prompt = \"what is a laptop?\"\n",
        "output = finetuned_generator(prompt,\n",
        "                   max_length=300,\n",
        "                   num_return_sequences=1,\n",
        "                   truncation=True,\n",
        "                   temperature=0.7)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4VuXnSmfhuY",
        "outputId": "014932cd-b79d-44ad-d671-ed030a047cff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is a laptop?\n",
            "\n",
            "A laptop is a laptop with a hard drive, keyboard, or other type of device connected to the internet such as a laptop computer, mobile device, or tablet. The laptop computer is a portable computer with a computer operating system such as Mac OS X, Windows, or Linux. A laptop computer is a computer that allows a user to view, browse, and interact with a variety of media. Examples of laptops include printers, e-readers, and mobile devices. A laptop computer is a laptop with a hard drive, keyboard, or other type of device connected to the internet such as a laptop computer, mobile device, or tablet.\n",
            "\n",
            "Is there a minimum amount of storage for a laptop?\n",
            "\n",
            "The following are the minimum storage requirements for a laptop:\n",
            "\n",
            "Minimum Storage: This percentage represents the total storage of the laptop, such as hard drive, hard disk drive, or other type of device. A laptop is not a laptop with a total of more than 100 GB of hard drive, hard disk drive, or other type of device.\n",
            "\n",
            "Maximum Storage: This percentage represents the amount of hard drive, hard disk drive, or other type of device that can be stored in a laptop. A laptop is not a laptop with a maximum of 100 GB of hard drive, hard disk drive, or other type of device.\n",
            "\n",
            "How to Calculate the Maximum Storage for a Device:\n",
            "\n",
            "Total Storage: A laptop is a laptop with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lruhsjeamXd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IYMd3Bz_mTIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P7FF21zUdTfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U7ko9my2dPks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Al8yu1r9dMrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLZKxeDbdKHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22qOZzZRcNsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LKT6tTlBZ6Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9WU_Y33-ZQNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nDa4g8WcZHua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KxxfW_B-ZAlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gb9gaI0EY8eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MClMrrV-Y2b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWiyJy8OYfR-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}